\documentclass{article}

\usepackage[
	backend=biber,
	%citestyle=authoryear,
	date=edtf,
	urldate=edtf,
	natbib=true
]{biblatex}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% TODO: Ensure that paragraphs have at least three sentences.

% -- Title stuff --
\title{Concept Suggestor}
\date{17th of May, 2017}
\author{Rapha\"el Claasen}

% -- Bibliography --
\addbibresource{bibliography.bib}

% -- Actual Document --
\begin{document}

\maketitle

\begin{abstract}
This is the abstract. Please write some more text here, I feel small and empty.
\end{abstract}

{\bf Keywords:} Natural Language Processing, semantic relatedness, semantic similarity

\section{Introduction} \label{sec:introduction}

Air Traffic Management (ATM) is a cross-disciplinary field that requires analysts from multiple domains such as security, safety, and business. When discussing problems about the ATM field, analysts often create domain-specific models that employ both shared and domain-specific concepts. To facilitate these discussions it is important that domain-specific models from one domain can be used in communication with other domains. This can be done by ensuring that domains use similar concepts in their models where possible.

It is difficult for analysts from different domains to communicate about their proposed solutions to a given ATM problem. This difficulty is partially caused by a difference in concepts used in their modeling which reduces the usefulness of these models in communication with other domains. When models are constructed well but use different concepts it can lead to different proposed solutions which have sound argumentation, but are hard to find a middle ground with.

To reduce this difficulty, suggestions can be made to modelers during the modeling process to ensure that similar concepts are used throughout all models. These suggestions can either be taken from previously created models or from a taxonomy relevant to the domain. 
It is important that any new concepts very similar to existing concepts in the models are not suggested. If ``Airplane" is already modeled, ``Aeroplane" should not be suggested as a new concept. For this, a system that compares new concepts to existing concepts and filters them based on similarity and synonymity can be used.

In practice, concepts used in modeling are not always single words. Multi-word concepts, such as ``Control Tower" will also need to be modeled. Natural Language Processing (NLP) does not yet reliably support comparing multi-word concepts. The relationship between the words in multi-word concepts depends significantly on the context, something computers usually find hard to understand. In the ATM context, however, the relationship between two-word concepts is almost always identical. We propose a way to measure similarity between two-word concepts.

\section{Related Work} \label{sec:relwork}

Natural Language Processing (NLP) is a field of computer science, computational linguistics, and artificial intelligence which concerns itself with the interaction between computers and (human) natural language. NLP can be applied in various fields of studies, such as text processing, speech recognition, artificial intelligence, machine translation, expert systems, and so on \citep{chowdhury2003natural}. 
In recent years, new technologies to model natural language through vector representation, such as word2vec and GloVe, have been developed \citep{mikolov2013efficient, pennington2014glove} and have shown that they perform well using the latest evaluation methods \citep{schnabel2015evaluation}. Of the two, GloVe performs slightly better than word2vec \citep{lee2016combining}.
One of the `older' ways of natural language processing is WordNet. WordNet is an electronical lexical database that includes various semantic relations between words, such as synonymy and hyponymy \citep{kilgarriff2000wordnet}.

SpaCy is one of the fastest and most accurate publically available NLP toolkits available through Python \citep{choi2015depends}. SpaCy uses GloVe, an unsupervised learning algorithm for constructing word vector representations for words by calculating word-word co-occurrence statistics from a corpus. Although co-occurrence is not identical to semantic similarity or relatedness, it can be used as a rough estimate of semantic similarity or relatedness \citep{levy2015improving}. SpaCy's default word vector library is trained on the Common Crawl corpus, an open repository of web crawl data that contains over 2.96 billion web pages and over 250 TiB of uncompressed content as of June 2017 \citep{nagel2017commoncrawl}.

The Natural Language Toolkit (NLTK) is a widely-used platform for interfacing with over 50 corpora and lexical resources, including WordNet, available through Python. WordNet stores lists of all the various senses of words. By looking for matching senses between two words, possible synonymity can be detected \cite{kilgarriff2000wordnet}. However, these senses are context-dependent. ``Sheet" and ``plane" have a matching sense, even though these words will not be synonyms in most contexts.

Constructing lexical databases and performing word vectorization are two ways to increase computer understanding of human language. Lee et al. have proven that combining these two methods into a weighted average increases the performance on a large amount of semantic relatedness measuring datasets. The weights they found to perform the best is in the range of $\lambda = 0.45-0.85$, where $\lambda$ is the weight of word vectorization, and $\mu = 1-\lambda$ is the weight of the lexical database \cite{lee2016combining}.

% WordNet is an electronic lexical database created by the Cognitive Science Laboratory of Princeton University starting in 1985. Since then, it has been updated and enhanced several times by researchers all over the world and is still maintained by Princeton University. % TODO: Determine whether to use this paragraph.

% Most methods of computing semantic relatedness measures such as word vectors rely on training an algorithm on a dataset. Public datasets, such as Google, Wikipedia, or WordNet are often used, although private datasets can also be used to improve context-sensitivity. Wikipedia and WordNet have proven to perform well in a variety of situations \citep{strube2006wikirelate}. % TODO: Determine whether to use this paragraph.

% Recommender systems are software tools and techniques providing suggestions for items to be of use to a user \citep{ricci2011introduction}. Recommender systems have become increasingly popular in recent years, and are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. There are also recommender systems for experts, collaborators, jokes, restaurants, garments, financial services, life insurance, romantic partners, and Twitter pages. % TODO: Ensure this is not plagiarism from https://en.wikipedia.org/wiki/Recommender_system % TODO: Determine whether to use this paragraph.

% This section lacks a clear structure. TODO: add clear structure. Add a paragraph that talks about NLP in general, its application areas, etc. I can add more methods/tools, also try to group the data sets. I need to mention word2vec algorithms as a baseline. NLTK is missing. When I cite a work, explain why it is relevant, what the advantages and limitations are of using it.

\section{Algorithms} \label{sec:algorithms}

Concept Suggestor aims to eliminate concepts that are too identical to each other. It is capable of doing this with concepts up to two words large. For this, it uses a pipeline constructed from several smaller algorithms that build upon each other. All algorithms depend on either SpaCy's implementation of the GloVe algorithm or NLTK's interface with WordNet.

\subsection{SpaCy and GloVe}
The similarity function of SpaCy, henceforth called $\textit{SpacySimilarity}$, computes the cosine of two word vectors. In section \ref{sec:relwork} we explained how these word vectors are constructed. $\textit{SpacySimilarity}$ returns a value between zero and one, where a value of zero indicates that two words are dissimilar, and a value of one indicates that two words are similar to each other. It is important to note that although the name of the function is $\textit{SpacySimilarity}$, $\textit{SpacySimilarity}$ actually measures semantic relatedness, not semantic similarity, due to the method used to construct the word vectors. 
% TODO: If the content is just one paragraph (or two) there is no need to create a subsection. Just keep the paragraph.

\subsection{NLTK and WordNet}
Although NLTK is an often-used library for NLP purposes, Concept Suggestor uses only the WordNet interface, so this subsection will focus primarily on WordNet. WordNet is a public electronic lexical database that began in the mid-1980s in the Princeton University Department of Psychology and has been regularly updated and improved since then. 
% The similarity function of NLTK, henceforth called $\textit{WordNetSimilarity}$ returns a score between zero and one, where a value of zero indicates that two words are dissimilar, and a value of one indicates that two words are similar to each other. This score is based on the shortest path that connects the senses using only hypernym or hyponym relationships.

NLTK offers six methods to calculate the similarity of two words \citep{pedersen2004wordnet}. Of the six, three use Information Content (IC), information about the meaning and usage of words \cite{seco2004intrinsic}. Four use the Least Common Subsumer (LCS), the most specific concept which is an ancestor of both words. The six methods are further explained and compared to each other below.
\begin{itemize}
	\item Path Similarity: Returns a score denoting how similar two word senses are, based on the shortest path that connects the senses traversing only hypernym and hyponym relationships. The score is in the range 0 to 1, where 0 indicates dissimilarity and 1 indicates similarity.
	\item Leacock-Chodorow Similarity: Returns a score denoting how similar two word senses are, based on the shortest path that connects the senses (as above) and the maximum depth of the taxonomy in which the senses occur. The relationship is given as \(-log(p/2d)\) where p is the shortest path length and d the taxonomy depth. % TODO: make this not copied from http://www.nltk.org/howto/wordnet.html
	\item Wu-Palmer Similarity: Returns a score denoting how similar two word senses are, based on the depth of the two senses in the taxonomy and that of the LCS.
	\item Resnik Similarity: Returns a score denoting how similar two word senses are, based on the IC of the LCS.
	\item Jiang-Conrath Similarity: Returns a score denoting how similar two word senses are, based on the IC of the LCS and that of the two input synsets. The relationship is given by equation \ref{eq:jcnsimilarity}, where $IC(X)$ stands for the information content of $X$, $s1$ and $s2$ stand for the synsets of the first and second word, and $lcs$ stands for the least common subsumer.
\begin{equation} \label{eq:jcnsimilarity}
	\frac{1} {IC(s1) + IC(s2) - 2 * IC(lcs)}
\end{equation}
	\item Lin Similarity: Returns a score denoting how similar two word senses are, based on the IC of the LCS and that of the two input Synsets. The relationship is given by equation \ref{eq:linsimilarity}. In equation \ref{eq:linsimilarity}, the same meanings for abbreviations are used as in equation \ref{eq:jcnsimilarity}, which can be found above.
\begin{equation} \label{eq:linsimilarity}
	\frac{2 * IC(lcs)} {IC(s1) + IC(s2)} 
\end{equation} 
\end{itemize}
% TODO: Ensure that all the items above are not too similar to their primary source, http://www.nltk.org/howto/wordnet.html

Table \ref{table:wordnetsimilarity} shows the results of each similarity measure on the two word pairs ``forest - graveyard" and ``asylum - madhouse". In table \ref{table:wordnetsimilarity}, the ``Human Estimate" rating is found from the Rubenstein-Goodenough set of word pairs \cite{rubenstein1965contextual}. In our case it is undesirable to use a similarity measure that uses Information Content. We would like to keep the possibility of training Concept Suggestor for specific domains using domain-related corpora, but Information Content is an old technology rarely supported by current NLP libraries, so using the Information Content from another corpus would require extensive work. Of the similarity measures that do not use Information Content, Wu-Palmer has the highest correlation with human estimates \citep{budanitsky2006evaluating,seco2004intrinsic,mihalcea2006corpus}.

\begin{table}[h!]
\centering
\begin{tabular}{|c||c|c|}
	\hline
	& forest - graveyard & asylum - madhouse \\
	\hline
	\textit{Human Estimate} & \textit{0.25} & \textit{0.76} \\
	Path & 0.0714 & 0.125 \\
	Leacock-Chodorow & 1.00 & 1.56 \\
	Wu-Palmer & 0.133 & 0.632 \\
	Resnik & 0.00 & 3.98 \\
	Jiang-Conrath & 0.0493 & 0.0661 \\
	Lin & 0.00 & 0.345 \\
	\hline
\end{tabular}
\caption{Comparison of WordNet similarity measures} % Human estimates taken from budanitsky2006evaluating
\label{table:wordnetsimilarity}
\end{table}

\subsection{The Pipeline}

Algorithms \ref{al:synonymity} and \ref{al:semanticsimilarity} are the two lowest-level algorithms. They determine synonymity and similarity of two words.

\begin{algorithm}
\caption{Determine synonymity of two words.}\label{al:synonymity}
\begin{algorithmic}[1]
	\Procedure{DetermineSynonymity}{wordA, wordB}
		\State $\textit{synsA} \gets \textit{WordNetSynsets}(wordA)$
		\State $\textit{synsB} \gets \textit{WordNetSynsets}(wordB)$
		\State \Return $\textit{HasOverlap}(synsA, synsB)$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Determine similarity of two words.}\label{al:semanticsimilarity}
\begin{algorithmic}[1]
	\Procedure{DetermineSimilarity}{wordA, wordB}
		\State $\textit{ss} \gets \textit{SpacySimilarity}(wordA, wordB)$
		\State $\textit{ws} \gets \textit{WordnetSimilarity}(wordA, wordB)$
		\State \Return $(\lambda*ss)+(\mu*ws)$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{al:synonymity} and \ref{al:semanticsimilarity} can be used to construct algorithm \ref{al:twowordpipeline}, the pipeline for two single-word concepts.

\begin{algorithm}
\caption{The pipeline on two single-word concepts.}\label{al:twowordpipeline}
\begin{algorithmic}[1]
	\Procedure{TwoWordPipeline}{wordA, wordB}
		\State $\textit{similarity} \gets \textit{DetermineSimilarity}(wordA, wordB)$
		\If {$\textit{similarity} > \textit{threshold}$}
			\If {$\textit{DetermineSynonymity}(wordA, wordB)$}
				\State \Return 1
			\EndIf
		\EndIf
		\State \Return $\textit{similarity}$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

Finally, algorithm \ref{al:twowordpipeline} can be used for building algorithm \ref{al:onetwocompoundpipeline}, an algorithm that compares a single-word concept to a two-word concept and for building algorithm \ref{al:twocompoundpipeline}, an algorithm that compares a two-word concept to another two-word concept.

\begin{algorithm}
\caption{The pipeline on one one-word concept and one two-word concept.}\label{al:onetwocompoundpipeline}
\begin{algorithmic}[1]
	\Procedure{OneTwoCompoundPipeline}{c1, c2w1, c2w2}
		\State $\textit{simA} \gets \textit{TwoWordPipeline}(c1, c2w2)$
		\State $\textit{simB} \gets \textit{TwoWordPipeline}(c1, c2w1)$
		\State \Return $\alpha * \textit{simA} + \beta * \textit{simB}$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{The pipeline on two two-word concepts.}\label{al:twocompoundpipeline}
\begin{algorithmic}[1]
	\Procedure{TwoCompoundPipeline}{c1w1, c1w2, c2w1, c2w2}
		\State $\textit{simA} \gets \textit{TwoWordPipeline}(c1w2, c2w2)$
		\State $\textit{simB} \gets \textit{TwoWordPipeline}(c1w1, c2w1)$
		\State $\textit{simC1} \gets \textit{TwoWordPipeline}(c1w1, c2w2)$
		\State $\textit{simC2} \gets \textit{TwoWordPipeline}(c1w2, c2w1)$
		\State \Return $\gamma * \textit{simA} + \delta * \textit{simB} + \epsilon * \textit{simC1} + \epsilon * \textit{simC2}$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

% ==============================================
% === EVERYTHING BELOW IS SUBJECT TO REMOVAL === Sort of, at least in this section.
% ==============================================

\subsection{Semantic Similarity of Two-Word Compounds}

To compute the overall semantic similarity of two-word compounds, the semantic similarity of each possible word-pair constructed with one word from each compound is calculated as above, and the weighted average of all these semantic similarities is computed.



\subsubsection{Comparing a Single Word to a Two-Word Compound}

When comparing a single word to a two-word compound, there are two possible word-pairs that can be constructed using one word from each compound. In example, when comparing the word ``Altitude" to ``Flight Level", the two word-pairs which can be constructed are ``Altitude-Flight" and ``Altitude-Level".
The formula used for computing the total similarity is 
\begin{equation} \label{eq:1wordto2word}
	\alpha * sim(c1, c1w2) + \beta * sim(c1, c1w1)
\end{equation}
where $\alpha$ and $\beta$ are the weights, $c1$ indicates the single word, and $c2w1$ and $c2w2$ are the first and second word of the two-word compound, respectively, and $sim(X,Y)$ computes the semantic similarity of $X$ and $Y$. % TODO: present this in a simpler way so people can easily understand it. Maybe write the compounds one after the other and enumerate each word (c1w1, c1w2, c2w1, c2w1 or c11, c12, c21 and c22) and update the formula. Always provide some examples, compare the similarity results and explain what it means.

We have found that weights of $\alpha = 0.67$ and $\beta = 0.33$ deliver acceptable accuracy. % TODO: actually test this one. % TODO: back this claim with some evidence.

\subsubsection{Comparing a Two-Word Compound to a Two-Word Compound}

When comparing a two-word compound to a two-word compound, there are four possible word-pairs that can be constructed using one word from each compound. In example, when comparing the compound ``Flight Level" to ``Emergency Altitude", the four word-pairs which can be constructed are ``Flight-Emergency", ``Flight-Altitude", ``Level-Emergency" and ``Level-Altitude".
The formula used for computing the total similarity is 
\begin{equation} \label{eq:2wordto2word}
	\alpha * sim(c1w2, c2w2) + \beta * sim(c1w1, c2w1) + \gamma * sim(c1w1, c2w2) + \gamma * sim(c1w2, c2w1)
\end{equation}
where $\alpha$, $\beta$, and $\gamma$ are the weights, $c1w1$ and $c1w2$ are the first and second word of the first compound, respectively, and $c2w1$ and $c2w2$ are the same for the second compound, and $sim(X,Y)$ computes the semantic similarity of $X$ and $Y$. 

We have found that weights of $\alpha = 0.4$, $\beta = 0.1$, and $\gamma = 0.25$ deliver acceptable accuracy.

\section{Evaluation Methodology} \label{sec:evalmeth}

In our estimates of threats to hidden biases in the compound terms, we have discovered four possible sources of bias:
\begin{itemize}
	\item Many compound terms have identical or similar first words.
	\item Many compound terms have identical or similar second words.
	\item Many compound terms have identical or similar diagonal relationships.
	\item Many compound terms have similar estimated similarity.
\end{itemize}
We have done our best to reduce these biases as much as possible, but keep in mind that no matter how careful a researcher can be, there are always threats to validity.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c||c|}
	\hline
	First Compound & Second Compound & Human Rating \\
	\hline
	Flight Level & Flight Altitude & ? \\
	Flight Attendant & Cabin Attendant & ? \\
	Control Tower & Ground Control & ? \\
	Passenger Airline & Passenger Aircraft & ? \\
	Computer Terminal & Flight Terminal & ? \\
	Police Officer & Security Officer & ? \\
	Emergency Flight & Emergency Altitude & ? \\
	Flight Course & Flight Plan & ? \\
	Emergency Flight & Emergency Plan & ? \\
	Business Plan & Emergency Plan & ? \\
	Airplane Wing & Aeroplane Wing & ? \\
	Business Plane & Business Sheet & ? \\
	Flight Terminal & International Flight & ? \\
	Intercontinental Flight & International Flight & ? \\
	Baggage Check & Sales Figure & ? \\
	Commercial Time & Revenue Loss & ? \\
	Paid Leave & Maternal Leave & ? \\
	Transit Airspace & Destination Aerodrome & ? \\
	Aeronautical Meteorology & Aviation Meteorology & ? \\
	Domestic Pilot & Flight Attendant & ? \\
	\hline
\end{tabular}
\caption{Human similarity rating of two-word compounds}
\label{table:humancompoundsimilarity}
\end{table}

% Just semantic similarity
Although there is no standard for computational evaluation of semantic similarity, there are generally three accepted methods \citep{meng2013review}.

Firstly, a theoretical examination of a computational measure can be made for those mathematical properties thought desirable, such as whether it is a metric, whether its parameter-projections are smooth functions, and so on. % TODO: Find out exactly what this means & rephrase it so I understand. It's quite complex language. Taken from meng2013review

Secondly, the coefficients of correlation with human judgement can be calculated, measured, and compared \citep{zhou2008new,seco2004intrinsic}.

Thirdly, if an application requires a measure of semantic similarity, we can compare the performance of different measures, while all other aspects of the application remain constant, to find the most effective measure \citep{blanchard2006tree,budanitsky2006evaluating}. % TODO: Explain each method in more detail.

% Semantic similarity and relatedness
The techniques for measuring semantic similarity and relatedness can be roughly categorized into two main categories \citep{agirre2009study}: Those that rely on pre-existing knowledge, such as thesauri, encyclopedias, and semantic networks \citep{alvarez2007graph,yang2005measuring,hughes2007lexical}, and those inducing distributional properties of words from corpora \citep{sahami2006web,chen2006novel,bollegala2007measuring}.


% Below is subject matter that I want to have handled in this section.
\begin{itemize}
	\item How to measure similarity in concepts used in the various domain-specific models?
	\item How to determine which are relevant concepts from other models that should be suggested to a modeler in order to improve the alignment between the models?
	\item How to determine which are relevant concepts from elsewhere that might improve alignment between the models?
\end{itemize}

\section{Experiments} \label{sec:experiments}

These are the experiments.

\section{Conclusions} \label{sec:conclusions}

These are the conclusions.

\section{Acknowledgements} \label{sec:ack}

I would like to thank Fabiano Dalpiaz and Ba\c sak Aydemir for sharing expertise, valuable guidance, and encouragement with me.

\printbibliography

\end{document}