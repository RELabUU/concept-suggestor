\documentclass{article}

\usepackage[
	backend=biber,
	%citestyle=authoryear,
	date=edtf,
	urldate=edtf,
	natbib=true
]{biblatex}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% TODO: Ensure that paragraphs have at least three sentences.

% -- Title stuff --
\title{Concept Suggestor}
\date{17th of May, 2017}
\author{Rapha\"el Claasen}

% -- Bibliography --
\addbibresource{bibliography.bib}

% -- Actual Document --
\begin{document}

\maketitle

\begin{abstract}
This is the abstract. Please write some more text here, I feel small and empty.
\end{abstract}

{\bf Keywords:} Natural Language Processing, semantic relatedness, semantic similarity

\section{Introduction} \label{sec:introduction}

Air Traffic Management (ATM) is a cross-disciplinary field that requires analysts from multiple domains such as security, safety, and business. When discussing problems about the ATM field, analysts often create domain-specific models that employ both shared and domain-specific concepts. To facilitate these discussions it is important that domain-specific models from one domain can be used in communication with other domains. This can be done by ensuring that domains use similar concepts in their models where possible.

It is difficult for analysts from different domains to communicate about their proposed solutions to a given ATM problem. This difficulty is partially caused by a difference in concepts used in their modeling which reduces the usefulness of these models in communication with other domains. When models are constructed well but use different concepts it can lead to different proposed solutions which have sound argumentation, but are hard to find a middle ground with.

To reduce this difficulty, suggestions can be made to modelers during the modeling process to ensure that similar concepts are used throughout all models. These suggestions can either be taken from previously created models or from a taxonomy relevant to the domain. 
It is important that any new concepts very similar to existing concepts in the models are not suggested. If ``Airplane" is already modeled, ``Aeroplane" should not be suggested as a new concept. For this, a system that compares new concepts to existing concepts and filters them based on similarity and synonymity can be used.

In practice, concepts used in modeling are not always single words. Multi-word concepts, such as ``Control Tower" will also need to be modeled. Natural Language Processing (NLP) does not yet reliably support comparing multi-word concepts. The relationship between the words in multi-word concepts depends significantly on the context, something computers usually find hard to understand. In the ATM context, however, the relationship between two-word concepts is almost always identical. We propose a way to measure similarity between two-word concepts.

\section{Related Work} \label{sec:relwork}

Natural Language Processing (NLP) is a field of computer science, computational linguistics, and artificial intelligence which concerns itself with the interaction between computers and (human) natural language. NLP can be applied in various fields of studies, such as text processing, speech recognition, artificial intelligence, machine translation, expert systems, and so on \citep{chowdhury2003natural}. 
In recent years, new technologies to model natural language through vector representation, such as word2vec and GloVe, have been developed \citep{mikolov2013efficient, pennington2014glove} and have shown that they perform well using the latest evaluation methods \citep{schnabel2015evaluation}. Of the two, GloVe performs slightly better than word2vec \citep{lee2016combining}.
One of the `older' ways of natural language processing is WordNet. WordNet is an electronical lexical database that includes various semantic relations between words, such as synonymy and hyponymy \citep{kilgarriff2000wordnet}.

SpaCy is one of the fastest and most accurate publically available NLP toolkits available through Python \citep{choi2015depends}. SpaCy uses GloVe, an unsupervised learning algorithm for constructing word vector representations for words by calculating word-word co-occurrence statistics from a corpus. Although co-occurrence is not identical to semantic similarity or relatedness, it can be used as a rough estimate of semantic similarity or relatedness \citep{levy2015improving}. SpaCy's default word vector library is trained on the Common Crawl corpus, an open repository of web crawl data that contains over 2.96 billion web pages and over 250 TiB of uncompressed content as of June 2017 \citep{nagel2017commoncrawl}.

The Natural Language Toolkit (NLTK) is a widely-used platform for interfacing with over 50 corpora and lexical resources, including WordNet, available through Python. WordNet stores lists of all the various senses of words. By looking for matching senses between two words, possible synonymity can be detected \cite{kilgarriff2000wordnet}. However, these senses are context-dependent. ``Sheet" and ``plane" have a matching sense, even though these words will not be synonyms in most contexts.

Constructing lexical databases and performing word vectorization are two ways to increase computer understanding of human language. Lee et al. have proven that combining these two methods into a weighted average increases the performance on a large amount of semantic relatedness measuring datasets. The weights they found to perform the best is in the range of $\lambda = 0.45-0.85$, where $\lambda$ is the weight of word vectorization, and $\mu = 1-\lambda$ is the weight of the lexical database \cite{lee2016combining}.

% WordNet is an electronic lexical database created by the Cognitive Science Laboratory of Princeton University starting in 1985. Since then, it has been updated and enhanced several times by researchers all over the world and is still maintained by Princeton University. % TODO: Determine whether to use this paragraph.

% Most methods of computing semantic relatedness measures such as word vectors rely on training an algorithm on a dataset. Public datasets, such as Google, Wikipedia, or WordNet are often used, although private datasets can also be used to improve context-sensitivity. Wikipedia and WordNet have proven to perform well in a variety of situations \citep{strube2006wikirelate}. % TODO: Determine whether to use this paragraph.

% Recommender systems are software tools and techniques providing suggestions for items to be of use to a user \citep{ricci2011introduction}. Recommender systems have become increasingly popular in recent years, and are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. There are also recommender systems for experts, collaborators, jokes, restaurants, garments, financial services, life insurance, romantic partners, and Twitter pages. % TODO: Ensure this is not plagiarism from https://en.wikipedia.org/wiki/Recommender_system % TODO: Determine whether to use this paragraph.

% This section lacks a clear structure. TODO: add clear structure. Add a paragraph that talks about NLP in general, its application areas, etc. I can add more methods/tools, also try to group the data sets. I need to mention word2vec algorithms as a baseline. NLTK is missing. When I cite a work, explain why it is relevant, what the advantages and limitations are of using it.

\section{Algorithms} \label{sec:algorithms}

Concept Suggestor aims to eliminate concepts that are too identical to each other. It is capable of doing this with concepts up to two words large. For this, it uses a pipeline constructed from several smaller algorithms that build upon each other. All algorithms depend on either SpaCy's implementation of the GloVe algorithm or NLTK's interface with WordNet.

\subsection{SpaCy and GloVe}
The similarity function of SpaCy, henceforth called $\textit{SpacySimilarity}$, computes the cosine of two word vectors. In section \ref{sec:relwork} we explained how these word vectors are constructed. $\textit{SpacySimilarity}$ returns a value between zero and one, where a value of zero indicates that two words are dissimilar, and a value of one indicates that two words are similar to each other. It is important to note that although the name of the function is $\textit{SpacySimilarity}$, $\textit{SpacySimilarity}$ actually measures semantic relatedness, not semantic similarity, due to the method used to construct the word vectors. 
% TODO: If the content is just one paragraph (or two) there is no need to create a subsection. Just keep the paragraph.

\subsection{NLTK and WordNet}
Although NLTK is an often-used library for NLP purposes, Concept Suggestor uses only the WordNet interface, so this subsection will focus primarily on WordNet. WordNet is a public electronic lexical database that began in the mid-1980s in the Princeton University Department of Psychology and has been regularly updated and improved since then. 
% The similarity function of NLTK, henceforth called $\textit{WordNetSimilarity}$ returns a score between zero and one, where a value of zero indicates that two words are dissimilar, and a value of one indicates that two words are similar to each other. This score is based on the shortest path that connects the senses using only hypernym or hyponym relationships.

NLTK offers six methods to calculate the similarity of two words \citep{pedersen2004wordnet}, henceforth called $\textit{WordnetSimilarity}$. Of the six, three use Information Content (IC), information about the meaning and usage of words \cite{seco2004intrinsic}. Four use the Least Common Subsumer (LCS), the most specific concept which is an ancestor of both words. The six methods are further explained and compared to each other below.
\begin{itemize}
	\item Path Similarity: Returns a score denoting how similar two word senses are, based on the shortest path that connects the senses traversing only hypernym and hyponym relationships. The score is in the range 0 to 1, where 0 indicates dissimilarity and 1 indicates similarity.
	\item Leacock-Chodorow Similarity: Returns a score denoting how similar two word senses are, based on the shortest path that connects the senses (as above) and the maximum depth of the taxonomy in which the senses occur. The relationship is given as \(-log(p/2d)\) where p is the shortest path length and d the taxonomy depth. % TODO: make this not copied from http://www.nltk.org/howto/wordnet.html
	\item Wu-Palmer Similarity: Returns a score denoting how similar two word senses are, based on the depth of the two senses in the taxonomy and that of the LCS.
	\item Resnik Similarity: Returns a score denoting how similar two word senses are, based on the IC of the LCS.
	\item Jiang-Conrath Similarity: Returns a score denoting how similar two word senses are, based on the IC of the LCS and that of the two input synsets. The relationship is given by equation \ref{eq:jcnsimilarity}, where $IC(X)$ stands for the information content of $X$, $s1$ and $s2$ stand for the synsets of the first and second word, and $lcs$ stands for the least common subsumer.
\begin{equation} \label{eq:jcnsimilarity}
	\frac{1} {IC(s1) + IC(s2) - 2 * IC(lcs)}
\end{equation}
	\item Lin Similarity: Returns a score denoting how similar two word senses are, based on the IC of the LCS and that of the two input Synsets. The relationship is given by equation \ref{eq:linsimilarity}. In equation \ref{eq:linsimilarity}, the same meanings for abbreviations are used as in equation \ref{eq:jcnsimilarity}, which can be found above.
\begin{equation} \label{eq:linsimilarity}
	\frac{2 * IC(lcs)} {IC(s1) + IC(s2)} 
\end{equation} 
\end{itemize}
% TODO: Ensure that all the items above are not too similar to their primary source, http://www.nltk.org/howto/wordnet.html

Table \ref{table:wordnetsimilarity} shows the results of each similarity measure on the two word pairs ``forest - graveyard" and ``asylum - madhouse". In table \ref{table:wordnetsimilarity}, the ``Human Estimate" rating is found from the Rubenstein-Goodenough set of word pairs \cite{rubenstein1965contextual}. In our case it is undesirable to use a similarity measure that uses Information Content. We would like to keep the possibility of training Concept Suggestor for specific domains using domain-related corpora, but Information Content is an old technology rarely supported by current NLP libraries, so using the Information Content from another corpus would require extensive work. Of the similarity measures that do not use Information Content, Wu-Palmer has the highest correlation with human estimates \citep{budanitsky2006evaluating,seco2004intrinsic,mihalcea2006corpus}. % TODO: Remove citations that do not support this statement.

\begin{table}[h!]
\centering
\begin{tabular}{|c||c|c|}
	\hline
	& forest - graveyard & asylum - madhouse \\
	\hline
	\textit{Human Estimate} & \textit{0.25} & \textit{0.76} \\
	Path & 0.0714 & 0.125 \\
	Leacock-Chodorow & 1.00 & 1.56 \\
	Wu-Palmer & 0.133 & 0.632 \\
	Resnik & 0.00 & 3.98 \\
	Jiang-Conrath & 0.0493 & 0.0661 \\
	Lin & 0.00 & 0.345 \\
	\hline
\end{tabular}
\caption{Comparison of WordNet similarity measures} % Human estimates taken from budanitsky2006evaluating
\label{table:wordnetsimilarity}
\end{table}

\subsection{The Pipeline}

The first building block of the pipeline, algorithm \ref{al:semanticsimilarity}, compares the similarity of two words to each other. It computes a similarity rating using both $\textit{SpacySimilarity}$ and $\textit{WordnetSimilarity}$ and returns a weighted average of both results, where $\lambda$ is the weight of $\textit{SpacySimilarity}$ and $\mu$ is the weight of $\textit{WordnetSimilarity}$. The result is between 0 and 1, where indicates the two words are dissimilar, and 1 indicates the two words are similar.

\begin{algorithm}
\caption{Determine similarity of two words.}\label{al:semanticsimilarity}
\begin{algorithmic}[1]
	\Procedure{DetermineSimilarity}{wordA, wordB}
		\State $\textit{ss} \gets \textit{SpacySimilarity}(wordA, wordB)$
		\State $\textit{ws} \gets \textit{WordnetSimilarity}(wordA, wordB)$
		\State \Return $(\lambda*ss)+(\mu*ws)$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

The second building block of the pipeline, algorithm \ref{al:synonymity}, determines whether two words are synonyms. It requests the synsets of both words from WordNet, and looks for any matching synset between the two. If a match is found, the two words are possibly synonyms and the algorithm returns \textit{true}. If no match is found, the algorithm returns \textit{false}.

\begin{algorithm}
\caption{Determine synonymity of two words.}\label{al:synonymity}
\begin{algorithmic}[1]
	\Procedure{DetermineSynonymity}{wordA, wordB}
		\State $\textit{synsA} \gets \textit{WordNetSynsets}(wordA)$
		\State $\textit{synsB} \gets \textit{WordNetSynsets}(wordB)$
		\State \Return $\textit{HasOverlap}(synsA, synsB)$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithms \ref{al:synonymity} and \ref{al:semanticsimilarity} can be used to construct algorithm \ref{al:twowordpipeline}, the pipeline for two single-word concepts.  When using only SpaCy and NLTK, words that are similar and also synonyms can have the same similarity score as words that are similar and not synonyms. Algorithm \ref{al:twowordpipeline} aims to ensure that similar words that are synonyms have the highest similarity score possible. If the similarity score from algorithm \ref{al:semanticsimilarity} is above a certain threshold, algorithm \ref{al:synonymity} is used to determine whether the two words are synonyms. The threshold ensures that words that are semantically dissimilar, but synonyms in certain contexts, such as ``plane" and ``sheet" do not get the highest similarity score possible. If the two words are synonyms, this algorithm changes their similarity score to 1. If the two words are not synonyms, the result from algorithm \ref{al:semanticsimilarity} is passed on. The aim of the threshold is only to filter out semantically dissimilar words. Because of this, a threshold of 0.6 suffices.

\begin{algorithm}
\caption{The pipeline on two single-word concepts.}\label{al:twowordpipeline}
\begin{algorithmic}[1]
	\Procedure{TwoWordPipeline}{wordA, wordB}
		\State $\textit{similarity} \gets \textit{DetermineSimilarity}(wordA, wordB)$
		\If {$\textit{similarity} > \textit{threshold}$}
			\If {$\textit{DetermineSynonymity}(wordA, wordB)$}
				\State \Return 1
			\EndIf
		\EndIf
		\State \Return $\textit{similarity}$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

Finally, algorithm \ref{al:twowordpipeline} can be used for building algorithm \ref{al:onetwocompoundpipeline} and \ref{al:twocompoundpipeline}, algorithms that determine the similarity score of compound concepts. Algorithm \ref{al:onetwocompoundpipeline} deterines the similarity between a one-word concept and a two-word concept. In the algorithm $c1$ stands for compound one, the one-word concept. $c2w1$ and $c2w2$ stand for the first and second word of the second compound. Algorithm \ref{al:onetwocompoundpipeline} uses two weights, $\alpha$ and $\beta$, which determine how significant the similarity between two words is for determining the final similarity score.

\begin{algorithm}
\caption{The pipeline on one one-word concept and one two-word concept.}\label{al:onetwocompoundpipeline}
\begin{algorithmic}[1]
	\Procedure{OneTwoCompoundPipeline}{c1, c2w1, c2w2}
		\State $\textit{simA} \gets \textit{TwoWordPipeline}(c1, c2w1)$
		\State $\textit{simB} \gets \textit{TwoWordPipeline}(c1, c2w2)$
		\State \Return $\alpha * \textit{simA} + \beta * \textit{simB}$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{al:twocompoundpipeline} determines the similarity between two two-word concepts. In the algorithm $c1w1$ and $c1w2$ stand for the first and second word of the first compound, and $c2w1$ and $c2w2$ for the first and second word of the second compound. Algorithm \ref{al:twocompoundpipeline} uses three weights, $\gamma$, $\delta$, and $\epsilon$, which determine how significant the similarity is between two words for determining the final similarity. $\gamma$ determines the weight of the similarity between $c1w1$ and $c2w1$, $\delta$ determines the weight of the similarity between $c1w2$ and $c2w2$, and $\epsilon$ determines the weight of the similarity between the diagonal relationships $c1w1$-$c2w2$ and $c1w2$-$c2w1$.

\begin{algorithm}
\caption{The pipeline on two two-word concepts.}\label{al:twocompoundpipeline}
\begin{algorithmic}[1]
	\Procedure{TwoCompoundPipeline}{c1w1, c1w2, c2w1, c2w2}
		\State $\textit{simC} \gets \textit{TwoWordPipeline}(c1w1, c2w1)$
		\State $\textit{simD} \gets \textit{TwoWordPipeline}(c1w2, c2w2)$
		\State $\textit{simE1} \gets \textit{TwoWordPipeline}(c1w1, c2w2)$
		\State $\textit{simE2} \gets \textit{TwoWordPipeline}(c1w2, c2w1)$
		\State \Return $\gamma * \textit{simC} + \delta * \textit{simD} + \epsilon * \textit{simE1} + \epsilon * \textit{simE2}$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Evaluation Methodology} \label{sec:evalmeth}

We aim to find the best values for $\lambda$, $\mu$, $\alpha$, $\beta$, $\gamma$, $\delta$, and $\epsilon$. To find the best values for $\gamma$, $\delta$, and $\epsilon$, we decided to test algorithm \ref{al:twocompoundpipeline} on a variety of two-word concept combinations, and compare the results to the similarity score estimated by human experts.

As there is very little research on the similarity of compound terms, there are no existing proven and tested datasets. For this research, we have had to create our own. In creating this dataset, we have determined four possible sources of bias:
\begin{itemize}
	\item Many compound terms have identical or similar first words.
	\item Many compound terms have identical or similar second words.
	\item Many compound terms have identical or similar diagonal relationships.
	\item Many compound terms have similar estimated similarity.
\end{itemize}
We have attempted to reduce these biases as much as possible in our dataset, which can be found below in table \ref{table:humancompoundsimilarity}.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c||c|}
	\hline
	First Compound & Second Compound & Expert Similarity Rating \\
	\hline
	Computer Terminal & Flight Terminal & 0.25 \\
	Business Plan & Emergency Plan & 0.25 \\
	Commercial Time & Revenue Loss & 0.00 \\
	Paid Leave & Maternal Leave & 0.75 \\
	Transit Airspace & Destination Aerodrome & 0.50 \\
	Flight Attendant & Cabin Attendant & 1.00 \\
	Intercontinental Flight & International Flight & 1.00 \\
	Aeronautical Meteorology & Aviation Meteorology & 1.00 \\
	Business Plane & Business Sheet & 1.00 \\
	Emergency Flight & Emergency Altitude & 0.75 \\
	Emergency Flight & Emergency Plan & 0.75 \\
	Passenger Airline & Passenger Aircraft & 0.50 \\
	Control Tower & Ground Control & 0.75 \\
	Baggage Check & Sales Figure & 0.00 \\
	Domestic Pilot & Flight Attendant & 0.75 \\
	Airplane Wing & Aeroplane Wing & 1.00 \\
	Flight Course & Flight Plan & 0.50 \\
	Police Officer & Security Officer & 0.75 \\
	Flight Level & Flight Altitude & 1.00 \\
	Flight Terminal & International Flight & 0.00 \\
	\hline
\end{tabular}
\caption{Human expert similarity rating of two-word compounds}
\label{table:humancompoundsimilarity}
\end{table}

Values for $\gamma$, $\delta$, and $\epsilon$ are needed to test values for $\lambda$ and $\mu$ and vice-versa. We have decided to first find the optimal values for $\gamma$, $\delta$, and $\epsilon$ using values of $\lambda = 0.65$ and $\mu = 0.35$ from Lee et al. as a starting point \cite{lee2016combining}, and afterwards find optimal values for $\lambda$ and $\mu$ using the results of the first experiment.

We will compare the Pearson correlation of algorithm \ref{al:twocompoundpipeline} with the dataset for various values of $\gamma$, $\delta$, and $\epsilon$. We will test all possible values of $\gamma$ and $\delta$ with intervals of 0.1 in the range 0..1. We will ensure that $\gamma + \delta + 2 * \epsilon = 1$ so that the total weighted similarity is in the range 0..1. All the tested values of $\gamma$, $\delta$, and $\epsilon$ can be found in table \ref{table:gammaresults1} and \ref{table:gammaresults2} in the appendix. 

After finding the optimal values of $\gamma$, $\delta$, and $\epsilon$, we will search for the optimal values of $\lambda$ and $\mu$ using the most and second most optimal values of $\gamma$, $\delta$ and $\epsilon$. We will compare the Pearson correlation of algorithm \ref{al:twocompoundpipeline} with the dataset for various values of $\lambda$ and $\mu$. We will test all possible values of $\lambda$ and $\mu$ with intervals of 0.1 in the range 0..1. We will ensure that $\lambda + \mu = 1$ so that the total weighted similarity is in the range 0..1. All the tested values of $\lambda$ and $\mu$ can be found in table \ref{table:lambdamuresults1}.

To properly estimate the efficacy of Concept Suggestor, after finding the optimal values for $\lambda$, $\mu$, $\gamma$, $\delta$, and $\epsilon$, we will compare the Pearson correlation of the similarity score found by the most optimal parameters with the average of the Pearson correlation of random similarity scores. We will determine the Pearson correlation of results where algorithm \ref{al:twocompoundpipeline} returns a random value in the 0..1 range twenty times and take the absolute value of each Pearson correlation so that negative correlations are properly counted. Finally, we will compare the Pearson correlation of Concept Suggestor to the Pearson correlation of random values.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% EVERYTHING BELOW SUBJECT TO REMOVAL %%% at least in Evaluation Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Just semantic similarity
%Although there is no standard for computational evaluation of semantic similarity, there are generally three accepted methods \citep{meng2013review}.

%Firstly, a theoretical examination of a computational measure can be made for those mathematical properties thought desirable, such as whether it is a metric, whether its parameter-projections are smooth functions, and so on. % TODO: Find out exactly what this means & rephrase it so I understand. It's quite complex language. Taken from meng2013review

%Secondly, the coefficients of correlation with human judgement can be calculated, measured, and compared \citep{zhou2008new,seco2004intrinsic}.

%Thirdly, if an application requires a measure of semantic similarity, we can compare the performance of different measures, while all other aspects of the application remain constant, to find the most effective measure \citep{blanchard2006tree,budanitsky2006evaluating}. % TODO: Explain each method in more detail.

% Semantic similarity and relatedness
%The techniques for measuring semantic similarity and relatedness can be roughly categorized into two main categories \citep{agirre2009study}: Those that rely on pre-existing knowledge, such as thesauri, encyclopedias, and semantic networks \citep{alvarez2007graph,yang2005measuring,hughes2007lexical}, and those inducing distributional properties of words from corpora \citep{sahami2006web,chen2006novel,bollegala2007measuring}.


% Below is subject matter that I want to have handled in this section.
%\begin{itemize}
%	\item How to measure similarity in concepts used in the various domain-specific models?
%	\item How to determine which are relevant concepts from other models that should be suggested to a modeler in order to improve the alignment between the models?
%	\item How to determine which are relevant concepts from elsewhere that might improve alignment between the models?
%\end{itemize}

\section{Experiments} \label{sec:experiments}

The experiments to find the best values for $\gamma$, $\delta$, and $\epsilon$ can be found in table \ref{table:gammaresults1} and \ref{table:gammaresults2} in the appendix. Values of $\gamma = 0.5$, $\delta = 0.4$, and $\epsilon = 0.05$ result in the highest Pearson correlation of 0.577. Values of $\gamma = 0.4$, $\delta = 0.4$, and $\epsilon = 0.1$ are in a very close second with a Pearson correlation of 0.576. 

The experiments to find the best values for $\lambda$ and $\mu$ can be found in table \ref{table:lambdamuresults1} and \ref{table:lambdamuresults2} below. Using the best and second best values for $\gamma$, $\delta$, and $\epsilon$, values of $\lambda = 1.0$ and $\mu = 0.0$ score the highest with a Pearson correlation of 0.613 and 0.597, respectively. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c||c|}
	\hline
	$\lambda$ & $\mu$ & Pearson Correlation \\
	\hline
	1.0 & 0.0 & 0.613 \\
	0.9 & 0.1 & 0.605 \\
	0.8 & 0.2 & 0.595 \\
	0.7 & 0.3 & 0.583 \\
	0.6 & 0.4 & 0.570 \\
	0.5 & 0.5 & 0.554 \\
	0.4 & 0.6 & 0.536 \\
	0.3 & 0.7 & 0.516 \\
	0.2 & 0.8 & 0.495 \\
	0.1 & 0.9 & 0.473 \\
	0.0 & 1.0 & 0.450 \\
	\hline
\end{tabular}
\caption{Experiment results for $\lambda$ and $\mu$ with $\gamma = 0.5$, $\delta = 0.4$, and $\epsilon = 0.05$}
\label{table:lambdamuresults1}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c||c|}
	\hline
	$\lambda$ & $\mu$ & Pearson Correlation \\
	\hline
	1.0 & 0.0 & 0.597 \\
	0.9 & 0.1 & 0.593 \\
	0.8 & 0.2 & 0.588 \\
	0.7 & 0.3 & 0.580 \\
	0.6 & 0.4 & 0.571 \\
	0.5 & 0.5 & 0.559 \\
	0.4 & 0.6 & 0.544 \\
	0.3 & 0.7 & 0.528 \\
	0.2 & 0.8 & 0.509 \\
	0.1 & 0.9 & 0.489 \\
	0.0 & 1.0 & 0.466 \\
	\hline
\end{tabular}
\caption{Experiment results for $\lambda$ and $\mu$ with $\gamma = 0.4$, $\delta = 0.4$, and $\epsilon = 0.1$}
\label{table:lambdamuresults2}
\end{table}

Using random similarity scores, we found an average Pearson correlation score of 0.145. More details can be found in table \ref{table:randomresults} in the appendix.

\section{Conclusions} \label{sec:conclusions}

The performance of Concept Suggestor is promising. Using the best parameters, a Pearson correlation score of 0.613 is reached with human expert estimates. Although it does not perform as well as many single-word concept similarity algorithms, it still performs significantly better than using random similarity scores. 

Concept Suggestor works better using just the similarity algorithm of SpaCy, and leaving the similarity algorithm of NLTK out.

Future research could investigate whether training SpaCy on a domain-specific corpus will improve the performance of Concept Suggestor. Future research could also establish a more peer tested dataset for testing multi-word compound similarity, which could significantly improve the validity of research on multi-word compound similarity.

\section{Acknowledgements} \label{sec:ack}

I would like to thank Fabiano Dalpiaz and Ba\c sak Aydemir for sharing expertise, valuable guidance, and encouragement with me. Special thanks to Fabiano Dalpiaz for sharing his expertise on the similarity of compound terms in the ATM domain.

\printbibliography

\section{Appendix}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c||c|}
	\hline
	$\gamma$ & $\delta$ & $\epsilon$ & Pearson Correlation \\
	\hline
	\hline
	0.0 & 0.0 & 0.5 & -0.076 \\
	0.0 & 0.1 & 0.45 & 0.003 \\
	0.0 & 0.2 & 0.4 & 0.093 \\
	0.0 & 0.3 & 0.35 & 0.167 \\
	0.0 & 0.4 & 0.3 & 0.215 \\ 
	0.0 & 0.5 & 0.25 & 0.243 \\
	0.0 & 0.6 & 0.2 & 0.259 \\
	0.0 & 0.7 & 0.15 & 0.267 \\
	0.0 & 0.8 & 0.1 & 0.272 \\
	0.0 & 0.9 & 0.05 & 0.275 \\
	0.0 & 1.0 & 0.0 & 0.277 \\
	\hline
	0.1 & 0.0 & 0.45 & 0.400 \\
	0.1 & 0.1 & 0.4 & 0.146 \\
	0.1 & 0.2 & 0.35 & 0.254 \\
	0.1 & 0.3 & 0.3 & 0.322 \\
	0.1 & 0.4 & 0.25 & 0.349 \\
	0.1 & 0.5 & 0.2 & 0.354 \\
	0.1 & 0.6 & 0.15 & 0.350 \\
	0.1 & 0.7 & 0.1 & 0.344 \\
	0.1 & 0.8 & 0.05 & 0.339 \\
	0.1 & 0.9 & 0.0 & 0.333 \\
	\hline
	0.2 & 0.0 & 0.4 & 0.163 \\
	0.2 & 0.1 & 0.35 & 0.290 \\
	0.2 & 0.2 & 0.3 & 0.407 \\
	0.2 & 0.3 & 0.25 & 0.463 \\
	0.2 & 0.4 & 0.2 & 0.470 \\
	0.2 & 0.5 & 0.15 & 0.454 \\
	0.2 & 0.6 & 0.1 & 0.434 \\
	0.2 & 0.7 & 0.05 & 0.416 \\
	0.2 & 0.8 & 0.0 & 0.400 \\
	\hline
	0.3 & 0.0 & 0.35 & 0.259 \\
	0.3 & 0.1 & 0.3 & 0.383 \\
	0.3 & 0.2 & 0.25 & 0.490 \\
	0.3 & 0.3 & 0.2 & 0.544 \\
	0.3 & 0.4 & 0.15 & 0.546 \\
	0.3 & 0.5 & 0.1 & 0.525 \\
	0.3 & 0.6 & 0.05 & 0.499 \\
	0.3 & 0.7 & 0.0 & 0.474 \\
	\hline
\end{tabular}
\caption{Experiment results for $\gamma$, $\delta$, and $\epsilon$ with $\lambda = 0.65$ and $\mu = 0.35$, part one}
\label{table:gammaresults1}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c||c|}
	\hline
	$\gamma$ & $\delta$ & $\epsilon$ & Pearson Correlation \\
	\hline
	\hline
	0.4 & 0.0 & 0.3 & 0.317 \\
	0.4 & 0.1 & 0.25 & 0.423 \\
	0.4 & 0.2 & 0.2 & 0.513 \\
	0.4 & 0.3 & 0.15 & 0.564 \\
	\textbf{0.4} & \textbf{0.4} & \textbf{0.1} & \textbf{0.576} \\
	0.4 & 0.5 & 0.05 & 0.563 \\
	0.4 & 0.6 & 0.0 & 0.540 \\
	\hline
	0.5 & 0.0 & 0.25 & 0.349 \\
	0.5 & 0.1 & 0.2 & 0.436 \\
	0.5 & 0.2 & 0.15 & 0.509 \\
	0.5 & 0.3 & 0.1 & 0.557 \\
	\textbf{0.5} & \textbf{0.4} & \textbf{0.05} & \textbf{0.577} \\
	0.5 & 0.5 & 0.0 & 0.574 \\
	\hline
	0.6 & 0.0 & 0.2 & 0.366 \\
	0.6 & 0.1 & 0.15 & 0.437 \\
	0.6 & 0.2 & 0.1 & 0.498 \\
	0.6 & 0.3 & 0.05 & 0.542 \\
	0.6 & 0.4 & 0.0 & 0.566 \\
	\hline
	0.7 & 0.0 & 0.15 & 0.376 \\
	0.7 & 0.1 & 0.1 & 0.435 \\
	0.7 & 0.2 & 0.05 & 0.486 \\
	0.7 & 0.3 & 0.0 & 0.526 \\
	\hline
	0.8 & 0.0 & 0.1 & 0.381 \\
	0.8 & 0.1 & 0.05 & 0.431 \\
	0.8 & 0.2 & 0.0 & 0.476 \\
	\hline
	0.9 & 0.0 & 0.05 & 0.384 \\
	0.9 & 0.1 & 0.0 & 0.427 \\
	\hline
	1.0 & 0.0 & 0.0 & 0.385 \\
	\hline
\end{tabular}
\caption{Experiment results for $\gamma$, $\delta$, and $\epsilon$ with $\lambda = 0.65$ and $\mu = 0.35$, part two}
\label{table:gammaresults2}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
	\hline
	Pearson Correlation & Absolute Value \\
	\hline
	0.106 & 0.106 \\
	0.045 & 0.045 \\
	-0.145 & 0.145 \\
	0.044 & 0.044 \\
	0.202 & 0.202 \\
	-0.051 & 0.051 \\
	-0.125 & 0.125 \\
	-0.053 & 0.053 \\
	-0.066 & 0.066 \\
	0.186 & 0.186 \\
	-0.201 & 0.201 \\
	-0.152 & 0.152 \\
	-0.195 & 0.195 \\
	0.166 & 0.166 \\
	0.333 & 0.333 \\
	-0.035 & 0.035 \\
	-0.029 & 0.029 \\
	-0.229 & 0.229 \\
	0.123 & 0.123 \\
	-0.421 & 0.421 \\
	\hline
	\hline
	Average & 0.145\\
	\hline
\end{tabular}
\caption{Pearson Correlation of random similarity scores to human expert estimates}
\label{table:randomresults}
\end{table}

\end{document}