\documentclass{article}

\usepackage[backend=biber]{biblatex}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% TODO: Ensure that paragraphs have at least three sentences.

% -- Title stuff --
\title{Concept Suggestor}
\date{17th of May, 2017}
\author{Rapha\"el Claasen}

% -- Bibliography --
\addbibresource{bibliography.bib}

% -- Actual Document --
\begin{document}

\maketitle

\begin{abstract}
This is the abstract.
\end{abstract}

{\bf Keywords:} NLP, semantic relatedness, semantic similarity

\section{Introduction}

Air Traffic Management (ATM) is a cross-disciplinary field that requires analysts from multiple domains such as security, safety, and business. When discussing problems about the ATM field, analysts often create domain-specific models that employ both shared and domain-specific concepts. To facilitate these discussions it is important that domain-specific models from one domain can be used in communication with other domains. This can be done by ensuring that domains use similar concepts in their models where possible.

It is difficult for analysts from different domains to communicate about their proposed solutions to a given ATM problem. This difficulty is partially caused by a difference in concepts used in their modeling which reduces the usefulness of these models in communication with other domains. When models are constructed well but use different concepts it can lead to different proposed solutions which have sound argumentation, but are hard to find a middle ground with.

To reduce this difficulty, suggestions can be made to modelers during the modeling process to ensure that similar concepts are used throughout all models. These suggestions can either be taken from previously created models or from a taxonomy relevant to the domain. 
It is important that any new concepts very similar to existing concepts in the models are not suggested. If "Airplane" is already modeled, "Aeroplane" should not be suggested as a new concept. For this, a system that compares new concepts to existing concepts and filters them based on similarity and synonymity can be used.

\section{Related Work}

Recommender systems are software tools and techniques providing suggestions for items to be of use to a user\cite{ricci2011introduction}. Recommender systems have become increasingly popular in recent years, and are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. There are also recommender systems for experts, collaborators, jokes, restaurants, garments, financial services, life insurance, romantic partners, and Twitter pages. % TODO: Ensure this is not plagiarism from https://en.wikipedia.org/wiki/Recommender_system

WordNet is an electronic lexical database\cite{kilgarriff2000wordnet}. % TODO: Add some more information on WordNet.
SpaCy is one of the fastest and most accurate publicly available Natural Language Processing (NLP) toolkits available\cite{choi2015depends}.
SpaCy uses GloVe to create word vectors. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus\cite{pennington2014glove}. Although GloVe measures co-occurrence and not semantic similarity or relatedness, its results can be used as a rough estimate of semantic similarity or relatedness\cite{levy2015improving}. SpaCy's default word vector library is trained on the Common Crawl corpus. The Common Crawl corpus contains over 2.96 billion web pages and over 250 TiB of uncompressed content as of June 2017\cite{nagel2017commoncrawl}.

Most methods of computing semantic relatedness measures such as word vectors rely on training an algorithm on a dataset. Public datasets, such as Google, Wikipedia, or WordNet are often used, although private datasets can also be used to improve context-sensitivity. Wikipedia and WordNet have proven to perform well in a variety of situations\cite{strube2006wikirelate}. % This section lacks a clear structure. TODO: add clear structure. Add a paragraph that talks about NLP in general, its application areas, etc. I can add more methods/tools, also try to group the data sets. I need to mention word2vec algorithms as a baseline. NLTK is missing. When I cite a work, explain why it is relevant, what the advantages and limitations are of using it.

\section{Algorithms}

The Concept Suggestor uses several steps to decide which concepts get suggested to the user. 

\begin{algorithm}
\caption{Determining synonymity of two words.}\label{al:synonymity}
\begin{algorithmic}[1]
	\Procedure{DetermineSynonymity}{wordA, wordB}
		\State $\textit{synsA} \gets \textit{wn.synsets}(wordA)$
		\State $\textit{synsB} \gets \textit{wn.synsets}(wordB)$
		\State \Return $\textit{HasOverlap}(synsA, synsB)$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{The pipeline on two single-word concepts.}\label{al:twowordpipeline}
\begin{algorithmic}[1]
	\Procedure{TwoWordPipeline}{wordA, wordB}
		\State $\textit{similarity} \gets \textit{DetermineSimilarity}(wordA, wordB)$
		\If {$\textit{similarity} > \textit{threshold}$}
			\If {$\textit{DetermineSynonymity}(wordA, wordB)$}
				\State \Return 1
			\EndIf
		\EndIf
		\State \Return $\textit{similarity}$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

% TODO: Add a high level description of the method I propose. There should be pseudo-code or a process diagram that explains the steps in detail.

\subsection{Semantic Similarity of two Words}

The first step is to determine semantic similarity of concepts. The Concept Suggestor combines two algorithms for this: SpaCy's similarity function and WordNet's similarity function. A weighted average of these two values is taken to determine overall word similarity. The following algorithm represents the similarity calculations: % TODO: Explain differences in the implementation. Also add some examples how the comparison results differ.
\begin{algorithm}
\caption{Determining semantic similarity of two words.}\label{al:semanticsimilarity}
\begin{algorithmic}[1]
	\Procedure{DetermineSimilarity}{wordA, wordB}
		\State $\textit{sw} \gets \text{Weight of SpaCy's similarity function.}$
		\State $\textit{ww} \gets \text{Weight of WordNet's similarity function.}$
		\State $\textit{ss} \gets \textit{spacySimilarity}(wordA, wordB)$
		\State $\textit{ws} \gets \textit{wordnetSimilarity}(wordA, wordB)$
		\State \Return $(sw*ss)+(ww*ws)$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{SpaCy}
SpaCy's word vectors are calculated by using the GloVe algorithm on the Common Crawl corpus. The similarity function of SpaCy computes the cosine of two word vectors. This results in a number between zero and one, where a value of zero indicates that two words are dissimilar, and a value of one indicates that two words are similar to each other. % TODO: If the content is just one paragraph (or two) there is no need to create a subsection. Just keep the paragraph.

\subsubsection{WordNet}
WordNet is a public electronic lexical database that began in the mid-1980s in the Princeton University Department of Psychology and has been regularly updated and improved since then. The similarity function of WordNet returns a score between zero and one denoting how similar two words are, based on the shortest path that connects the senses in the is-a (hypernym/hyponym) taxonomy. % TODO: make the last sentence not plagiarism. Source: http://www.nltk.org/howto/wordnet.html 

WordNet offers six methods to measure similarity\cite{pedersen2004wordnet}. % TODO: Select one or two pairs of words and calculate the results for these different methods and show the results in a table.
\begin{itemize}
	\item Path Similarity: Returns a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hyponym) taxonomy. The score is in the range 0 to 1. % TODO: make this not copied from http://www.nltk.org/howto/wordnet.html
	\item Leacock-Chodorow Similarity: Returns a score denoting how similar two word senses are, based on the shortest path that connects the senses (as above) and the maximum depth of the taxonomy in which the senses occur. The relationship is given as \(-log(p/2d)\) where p is the shortest path length and d the taxonomy depth. % TODO: make this not copied from http://www.nltk.org/howto/wordnet.html
	\item Wu-Palmer Similarity: Returns a score denoting how similar two word senses are, based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer (most specific ancestor node). % TODO: make this not copied from http://www.nltk.org/howto/wordnet.html
	\item Resnik Similarity: Returns a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node). % TODO: make this not copied from http://www.nltk.org/howto/wordnet.html
	\item Jiang-Conrath Similarity: Returns a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input synsets. The relationship is given by the following equation:
\begin{equation} \label{eq:jcnsimilarity}
	\frac{1} {IC(s1) + IC(s1) - 2 * IC(lcs)}
\end{equation}
% TODO: make this not copied from http://www.nltk.org/howto/wordnet.html
	\item Lin Similarity: Returns a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. The relationship is given by the follwing equation:
\begin{equation} \label{eq:linsimilarity}
	\frac{2 * IC(lcs)} {IC(s1) + IC(s2)} 
\end{equation} 
% TODO: make this not copied from http://www.nltk.org/howto/wordnet.html
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|c||c|c|}
	\hline
	& forest - graveyard & asylum - madhouse \\
	\hline
	Human Estimate & 0.25 & 0.76 \\
	Path & 0.0714 & 0.125 \\
	Leacock-Chodorow & 1.00 & 1.56 \\
	Wu-Palmer & 0.133 & 0.632 \\
	Resnik & 0.00 & 3.98 \\
	Jiang-Conrath & 0.0493 & 0.0661 \\
	Lin & 0.00 & 0.345 \\
	\hline
\end{tabular}
\caption{Comparison of WordNet similarity measures} % Human estimates taken from budanitsky2006evaluating
\label{table:wordnetsimilarity}
\end{table}

From these methods, the Wu-Palmer method is a good balance between performance and accuracy\cite{budanitsky2006evaluating,seco2004intrinsic,mihalcea2006corpus}.

\subsection{Semantic Similarity of Two-Word Compounds}

To compute the overall semantic similarity of two-word compounds, the semantic similarity of each possible word-pair constructed with one word from each compound is calculated as above, and the weighted average of all these semantic similarities is computed.

In our estimates of threats to hidden biases in the compound terms, we have discovered four possible sources of bias:
\begin{itemize}
	\item Many compound terms have identical or similar first words.
	\item Many compound terms have identical or similar second words.
	\item Many compound terms have identical or similar diagonal relationships.
	\item Many compound terms have similar estimated similarity.
\end{itemize}
We have done our best to reduce these biases as much as possible, but keep in mind that no matter how careful a researcher can be, there are always threats to validity.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c||c|}
	\hline
	First Compound & Second Compound & Human Rating \\
	\hline
	Flight Level & Flight Altitude & ? \\
	Flight Attendant & Cabin Attendant & ? \\
	Control Tower & Ground Control & ? \\
	Passenger Airline & Passenger Aircraft & ? \\
	Computer Terminal & Flight Terminal & ? \\
	Police Officer & Security Officer & ? \\
	Emergency Flight & Emergency Altitude & ? \\
	Flight Course & Flight Plan & ? \\
	Emergency Flight & Emergency Plan & ? \\
	Business Plan & Emergency Plan & ? \\
	Airplane Wing & Aeroplane Wing & ? \\
	Business Plane & Business Sheet & ? \\
	Flight Terminal & International Flight & ? \\
	Intercontinental Flight & International Flight & ? \\
	Baggage Check & Sales Figure & ? \\
	Commercial Time & Revenue Loss & ? \\
	Paid Leave & Maternal Leave & ? \\
	Transit Airspace & Destination Aerodrome & ? \\
	Aeronautical Meteorology & Aviation Meteorology & ? \\
	Domestic Pilot & Flight Attendant & ? \\
	\hline
\end{tabular}
\caption{Human similarity rating of two-word compounds}
\label{table:humancompoundsimilarity}
\end{table}

\subsubsection{Comparing a Single Word to a Two-Word Compound}

When comparing a single word to a two-word compound, there are two possible word-pairs that can be constructed using one word from each compound. In example, when comparing the word "Altitude" to "Flight Level", the two word-pairs which can be constructed are "Altitude-Flight" and "Altitude-Level".
The formula used for computing the total similarity is 
\begin{equation} \label{eq:1wordto2word}
	\alpha * sim(c1, c1w2) + \beta * sim(c1, c1w1)
\end{equation}
where $\alpha$ and $\beta$ are the weights, $c1$ indicates the single word, and $c2w1$ and $c2w2$ are the first and second word of the two-word compound, respectively, and $sim(X,Y)$ computes the semantic similarity of $X$ and $Y$. % TODO: present this in a simpler way so people can easily understand it. Maybe write the compounds one after the other and enumerate each word (c1w1, c1w2, c2w1, c2w1 or c11, c12, c21 and c22) and update the formula. Always provide some examples, compare the similarity results and explain what it means.

\begin{algorithm}
\caption{The pipeline on one one-word concept and one two-word concept.}\label{al:onetwocompoundpipeline}
\begin{algorithmic}[1]
	\Procedure{OneTwoCompoundPipeline}{c1, c2w1, c2w2}
		\State $\textit{simA} \gets \textit{TwoWordPipeline}(c1, c2w2)$
		\State $\textit{simB} \gets \textit{TwoWordPipeline}(c1, c2w1)$
		\State \Return $\alpha * \textit{simA} + \beta * \textit{simB}$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

We have found that weights of $\alpha = 0.67$ and $\beta = 0.33$ deliver acceptable accuracy. % TODO: actually test this one. % TODO: back this claim with some evidence.

\subsubsection{Comparing a Two-Word Compound to a Two-Word Compound}

When comparing a two-word compound to a two-word compound, there are four possible word-pairs that can be constructed using one word from each compound. In example, when comparing the compound "Flight Level" to "Emergency Altitude", the four word-pairs which can be constructed are "Flight-Emergency", "Flight-Altitude", "Level-Emergency" and "Level-Altitude".
The formula used for computing the total similarity is 
\begin{equation} \label{eq:2wordto2word}
	\alpha * sim(c1w2, c2w2) + \beta * sim(c1w1, c2w1) + \gamma * sim(c1w1, c2w2) + \gamma * sim(c1w2, c2w1)
\end{equation}
where $\alpha$, $\beta$, and $\gamma$ are the weights, $c1w1$ and $c1w2$ are the first and second word of the first compound, respectively, and $c2w1$ and $c2w2$ are the same for the second compound, and $sim(X,Y)$ computes the semantic similarity of $X$ and $Y$. 

\begin{algorithm}
\caption{The pipeline on two two-word concepts.}\label{al:twocompoundpipeline}
\begin{algorithmic}[1]
	\Procedure{TwoCompoundPipeline}{c1w1, c1w2, c2w1, c2w2}
		\State $\textit{simA} \gets \textit{TwoWordPipeline}(c1w2, c2w2)$
		\State $\textit{simB} \gets \textit{TwoWordPipeline}(c1w1, c2w1)$
		\State $\textit{simC1} \gets \textit{TwoWordPipeline}(c1w1, c2w2)$
		\State $\textit{simC2} \gets \textit{TwoWordPipeline}(c1w2, c2w1)$
		\State \Return $\alpha * \textit{simA} + \beta * \textit{simB} + \gamma * \textit{simC1} + \gamma * \textit{simC2}$
	\EndProcedure
\end{algorithmic}
\end{algorithm}

We have found that weights of $\alpha = 0.4$, $\beta = 0.1$, and $\gamma = 0.25$ deliver acceptable accuracy.

\section{Evaluation Methodology}

% Just semantic similarity
Although there is no standard for computational evaluation of semantic similarity, there are generally three accepted methods\cite{meng2013review}.

Firstly, a theoretical examination of a computational measure can be made for those mathematical properties thought desirable, such as whether it is a metric, whether its parameter-projections are smooth functions, and so on. % TODO: Find out exactly what this means & rephrase it so I understand. It's quite complex language. Taken from meng2013review

Secondly, the coefficients of correlation with human judgement can be calculated, measured, and compared\cite{zhou2008new,seco2004intrinsic}.

Thirdly, if an application requires a measure of semantic similarity, we can compare the performance of different measures, while all other aspects of the application remain constant, to find the most effective measure\cite{blanchard2006tree,budanitsky2006evaluating}. % TODO: Explain each method in more detail.

% Semantic similarity and relatedness
The techniques for measuring semantic similarity and relatedness can be roughly categorized into two main categories\cite{agirre2009study}: Those that rely on pre-existing knowledge, such as thesauri, encyclopedias, and semantic networks\cite{alvarez2007graph,yang2005measuring,hughes2007lexical}, and those inducing distributional properties of words from corpora\cite{sahami2006web,chen2006novel,bollegala2007measuring}.


% Below is subject matter that I want to have handled in this section.
\begin{itemize}
	\item How to measure similarity in concepts used in the various domain-specific models?
	\item How to determine which are relevant concepts from other models that should be suggested to a modeler in order to improve the alignment between the models?
	\item How to determine which are relevant concepts from elsewhere that might improve alignment between the models?
\end{itemize}

\section{Experiments}

These are the experiments.

\section{Conclusions}

These are the conclusions.

\section{Acknowledgements}

I would like to thank Fabiano Dalpiaz and Ba\c sak Aydemir for sharing expertise, valuable guidance, and encouragement with me.

\printbibliography

\end{document}